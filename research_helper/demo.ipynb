{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%pip install -qU langchain langchain-community langchain-openai chromadb arxiv tiktoken pymupdf #opentelemetry-sdk==1.31.0"
      ],
      "metadata": {
        "id": "8rIpqAE-qJG4"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"] ='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'\n",
        "LLM_MODEL = \"gpt-4o-mini\"\n",
        "EMBED_MODEL = \"text-embedding-3-large\"\n",
        "CHROMA_DIR = \"./chroma_arxiv\"\n",
        "COLLECTION_NAME = \"arxiv_top_tier\""
      ],
      "metadata": {
        "id": "-P1VvuSwqJJB"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.retrievers import ArxivRetriever\n",
        "import re\n",
        "import arxiv\n",
        "import fitz\n",
        "\n",
        "def build_top_tier_query(user_query: str) -> str:\n",
        "    venues = ['(NeurIPS)','(ICML)', '(ICLR)', '(ACL)']#, '(CVPR)', '(ICCV)', '(ECCV)', '(AAAI)', '(KDD)', '(ACL)', '(EMNLP)']\n",
        "    venue_filter = \" OR \".join([f\"jr:{v} OR co:{v}\" for v in venues])\n",
        "    #return f\"({venue_filter}) AND ({user_query})\"\n",
        "    return f\"({user_query})\"\n",
        "\n",
        "\n",
        "retriever = ArxivRetriever(\n",
        "    load_max_docs=15,\n",
        "    doc_content_chars_max=30000\n",
        ")\n",
        "\n",
        "def extract_common_fields_from_doc(d):\n",
        "    m = d.metadata or {}\n",
        "    title     = m.get(\"title\") or m.get(\"Title\")  # pretty_print_docs와 동일\n",
        "    url       = m.get(\"entry_id\") or m.get(\"Entry ID\") or m.get(\"Entry_ID\") or m.get(\"url\") or m.get(\"pdf_url\")\n",
        "    authors   = m.get(\"Authors\")\n",
        "    published = str(m.get(\"Published\") or m.get(\"published\") or m.get(\"publish_date\"))\n",
        "    content   = (d.page_content or \"\").strip()\n",
        "    return title, url, authors, published, content\n",
        "\n",
        "\n",
        "def pretty_print_docs(docs, max_chars=1200):\n",
        "    lines = []\n",
        "    for i, d in enumerate(docs, 1):\n",
        "        title, url, authors, published, content = extract_common_fields_from_doc(d)\n",
        "        if len(content) > max_chars:\n",
        "            content = content[:max_chars] + \" ...\"\n",
        "        block = [\n",
        "            f\"### [{i}] {title}\",\n",
        "            f\"- url: {url}\",\n",
        "            f\"- published: {published}\",\n",
        "            f\"- authors: {authors}\",\n",
        "            f\"- content:\\n{content}\",\n",
        "        ]\n",
        "        lines.append(\"\\n\".join(block))\n",
        "    return \"\\n\\n\".join(lines)\n",
        "\n",
        "def extract_arxiv_id_from_url(url: str) -> str:\n",
        "    if not url:\n",
        "        return \"\"\n",
        "    m = re.search(r'arxiv\\.org/(abs|pdf)/([0-9]+\\.[0-9]+)', url)\n",
        "    return m.group(2) if m else \"\"\n",
        "\n",
        "def fetch_full_text_from_arxiv_id(arxiv_id: str, char_limit: int = None) -> str:\n",
        "    if not arxiv_id:\n",
        "        return \"\"\n",
        "    search = arxiv.Search(id_list=[arxiv_id])\n",
        "    client = arxiv.Client()\n",
        "    results = list(client.results(search))\n",
        "    if not results:\n",
        "        return \"\"\n",
        "    pdf_path = results[0].download_pdf()\n",
        "    text = \"\"\n",
        "    with fitz.open(pdf_path) as doc:\n",
        "        text = \"\".join(page.get_text() for page in doc)\n",
        "    if char_limit is not None and len(text) > char_limit:\n",
        "        text = text[:char_limit]\n",
        "    return text.strip()"
      ],
      "metadata": {
        "id": "4xiNESGua4Pi"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain_text_splitters import TokenTextSplitter\n",
        "from langchain_community.vectorstores import Chroma\n",
        "\n",
        "embeddings = OpenAIEmbeddings(model=EMBED_MODEL)\n",
        "text_splitter = TokenTextSplitter(\n",
        "    chunk_size=3000, chunk_overlap=1000\n",
        ")\n",
        "vectorstore = Chroma(\n",
        "    collection_name=COLLECTION_NAME,\n",
        "    embedding_function=embeddings,\n",
        "    persist_directory=CHROMA_DIR,\n",
        ")\n",
        "\n",
        "def upsert_into_chroma(docs, fulltext_char_limit: int = None):\n",
        "    texts, metadatas, ids = [], [], []\n",
        "    for d in docs:\n",
        "        title, url, authors, published, abstract = extract_common_fields_from_doc(d)\n",
        "        arxiv_id = extract_arxiv_id_from_url(url) or (re.sub(r'\\W+', '-', str(title))[:50] if title else \"\")\n",
        "\n",
        "        # PDF 전문 텍스트 우선 수집 (실패 시 초록으로 폴백)\n",
        "        full_text = \"\"\n",
        "        try:\n",
        "            full_text = fetch_full_text_from_arxiv_id(arxiv_id, char_limit=fulltext_char_limit)\n",
        "        except Exception:\n",
        "            full_text = \"\"\n",
        "        if not full_text:\n",
        "            full_text = abstract or \"\"\n",
        "        if not full_text.strip():\n",
        "            continue\n",
        "\n",
        "        chunks = text_splitter.split_text(full_text)\n",
        "        for j, chunk in enumerate(chunks):\n",
        "            texts.append(chunk)\n",
        "            metadatas.append({\n",
        "                \"paper_title\": title,\n",
        "                \"paper_url\": url,\n",
        "                \"authors\": authors,\n",
        "                \"published\": published,\n",
        "                \"arxiv_id\": arxiv_id,\n",
        "                \"chunk_index\": j,\n",
        "            })\n",
        "            ids.append(f\"{arxiv_id}::{j}\")\n",
        "\n",
        "    if texts:\n",
        "        vectorstore.add_texts(texts=texts, metadatas=metadatas, ids=ids)\n",
        "        vectorstore.persist()\n",
        "\n",
        "def run_arxiv_search_with_top_tier(user_query: str):\n",
        "    query = build_top_tier_query(user_query)\n",
        "    docs = retriever.invoke(query)\n",
        "    rendered = pretty_print_docs(docs)\n",
        "    upsert_into_chroma(docs, fulltext_char_limit=None)\n",
        "    return docs, rendered"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6W6NC_v1a4Sd",
        "outputId": "2798911e-23a5-46eb-d030-513c5dbb502c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2150345550.py:9: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
            "  vectorstore = Chroma(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "llm = ChatOpenAI(model=LLM_MODEL, temperature=1)\n",
        "\n",
        "summary_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\",\n",
        "     \"You are an expert AI research summarizer. Using the provided arXiv documents, \"\n",
        "     \"first write a concise overview of the research landscape related to the user's query. \"\n",
        "     \"Then provide a bullet list of references with: [Title](URL) — Published — Authors. \"\n",
        "     \"The base language is Korean for explanations (not titles/authors/model names).\"),\n",
        "    (\"human\",\n",
        "     \"User query:\\n{user_query}\\n\\nDocuments:\\n{docs_rendered}\")\n",
        "])\n",
        "\n",
        "summary_chain = summary_prompt | llm | StrOutputParser()"
      ],
      "metadata": {
        "id": "03kNYUXqa4X9"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.runnables import RunnableParallel, RunnableLambda\n",
        "\n",
        "local_retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
        "\n",
        "rag_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\",\n",
        "     \"Answer the user's question using ONLY the provided context chunks from previously saved papers. \"\n",
        "     \"Cite the paper titles inline when relevant. If not in context, say you don't have it.\"),\n",
        "    (\"human\", \"Question: {question}\\n\\nContext:\\n{context}\")\n",
        "])\n",
        "\n",
        "def format_docs(docs):\n",
        "    lines = []\n",
        "    for d in docs:\n",
        "        m = d.metadata or {}\n",
        "        title = m.get(\"paper_title\", \"(unknown)\")\n",
        "        lines.append(f\"[{title}] (chunk {m.get('chunk_index', '?')}):\\n{d.page_content}\\n\")\n",
        "    return \"\\n\\n\".join(lines)\n",
        "\n",
        "rag_chain = (\n",
        "    {\"context\": local_retriever | RunnableLambda(format_docs), \"question\": RunnableLambda(lambda x: x)}\n",
        "    | rag_prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")"
      ],
      "metadata": {
        "id": "X2dmQXwWqJWr"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================\n",
        "# 첫 질의 파이프라인\n",
        "# ================================\n",
        "def run_pipeline(user_query: str):\n",
        "    docs, rendered = run_arxiv_search_with_top_tier(user_query)\n",
        "    print(\"=== [Retrieved Docs for Agent] ===\\n\")  # UPDATED\n",
        "    print(rendered)\n",
        "\n",
        "    print(\"\\n\\n=== [Summarized Overview + References] ===\\n\")\n",
        "    overview = summary_chain.invoke({\"user_query\": user_query, \"docs_rendered\": rendered})\n",
        "    print(overview)\n",
        "    # 후속판단용으로 '추천 7편의 제목 목록' 보관\n",
        "    global RECOMMENDED_TITLES\n",
        "    RECOMMENDED_TITLES = []\n",
        "    for d in docs:\n",
        "        t, _, _, _, _ = extract_common_fields_from_doc(d)\n",
        "        if t:\n",
        "            RECOMMENDED_TITLES.append(t)\n",
        "    return #overview\n",
        "\n",
        "# ================================\n",
        "# 후속 질의용 Agent 라우팅\n",
        "# ================================\n",
        "route_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\",\n",
        "     \"You are a router. Decide whether the user's follow-up question is primarily about any of the following papers. \"\n",
        "     \"If yes, answer EXACTLY 'RAG' (and list related titles after a pipe), else answer EXACTLY 'NO_RAG'. \"\n",
        "     \"Do not add extra words.\\n\\nPapers:\\n{titles}\"),\n",
        "    (\"human\", \"Question: {question}\")\n",
        "])\n",
        "\n",
        "route_chain = route_prompt | llm | StrOutputParser()\n",
        "\n",
        "nonrag_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\",\n",
        "     \"You are a helpful AI research assistant. Answer the question concisely in Korean. \"\n",
        "     \"Do not assume access to local papers.\"),\n",
        "    (\"human\", \"{question}\")\n",
        "])\n",
        "nonrag_chain = nonrag_prompt | llm | StrOutputParser()\n",
        "\n",
        "def ask_followup_agent(question: str):\n",
        "    titles_str = \"\\n\".join(f\"- {t}\" for t in globals().get(\"RECOMMENDED_TITLES\", []))\n",
        "    decision = route_chain.invoke({\"titles\": titles_str, \"question\": question}).strip()\n",
        "    use_rag = decision.startswith(\"RAG\")\n",
        "    print(f\"[Router] decision = {decision}\")\n",
        "\n",
        "    if use_rag:\n",
        "        print(\"=== [RAG Answer From Local Chroma] ===\\n\")\n",
        "        ans = rag_chain.invoke(question)\n",
        "    else:\n",
        "        print(\"=== [Non-RAG LLM Answer] ===\\n\")\n",
        "        ans = nonrag_chain.invoke({\"question\": question})\n",
        "    print(ans)\n",
        "    return #ans"
      ],
      "metadata": {
        "id": "DSieHqYIdS_E"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "run_pipeline(\"CT denoising\")\n"
      ],
      "metadata": {
        "id": "dkhMM78iqJZI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac53b53d-f576-4fa6-f981-062856cfdc99"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2150345550.py:47: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
            "  vectorstore.persist()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== [Retrieved Docs for Agent] ===\n",
            "\n",
            "### [1] Self-Supervised Learning based CT Denoising using Pseudo-CT Image Pairs\n",
            "- url: http://arxiv.org/abs/2104.02326v1\n",
            "- published: 2021-04-06\n",
            "- authors: Dongkyu Won, Euijin Jung, Sion An, Philip Chikontwe, Sang Hyun Park\n",
            "- content:\n",
            "Recently, Self-supervised learning methods able to perform image denoising\n",
            "without ground truth labels have been proposed. These methods create\n",
            "low-quality images by adding random or Gaussian noise to images and then train\n",
            "a model for denoising. Ideally, it would be beneficial if one can generate\n",
            "high-quality CT images with only a few training samples via self-supervision.\n",
            "However, the performance of CT denoising is generally limited due to the\n",
            "complexity of CT noise. To address this problem, we propose a novel\n",
            "self-supervised learning-based CT denoising method. In particular, we train\n",
            "pre-train CT denoising and noise models that can predict CT noise from Low-dose\n",
            "CT (LDCT) using available LDCT and Normal-dose CT (NDCT) pairs. For a given\n",
            "test LDCT, we generate Pseudo-LDCT and NDCT pairs using the pre-trained\n",
            "denoising and noise models and then update the parameters of the denoising\n",
            "model using these pairs to remove noise in the test LDCT. To make realistic\n",
            "Pseudo LDCT, we train multiple noise models from individual images and generate\n",
            "the noise using the ensemble of noise models. We evaluate our method on the\n",
            "2016 AAPM Low-Dose CT Grand Challenge dataset. The proposed ensemble noi ...\n",
            "\n",
            "### [2] Low-dose CT Denoising with Language-engaged Dual-space Alignment\n",
            "- url: http://arxiv.org/abs/2403.06128v1\n",
            "- published: 2024-03-10\n",
            "- authors: Zhihao Chen, Tao Chen, Chenhui Wang, Chuang Niu, Ge Wang, Hongming Shan\n",
            "- content:\n",
            "While various deep learning methods were proposed for low-dose computed\n",
            "tomography (CT) denoising, they often suffer from over-smoothing, blurring, and\n",
            "lack of explainability. To alleviate these issues, we propose a plug-and-play\n",
            "Language-Engaged Dual-space Alignment loss (LEDA) to optimize low-dose CT\n",
            "denoising models. Our idea is to leverage large language models (LLMs) to align\n",
            "denoised CT and normal dose CT images in both the continuous perceptual space\n",
            "and discrete semantic space, which is the first LLM-based scheme for low-dose\n",
            "CT denoising. LEDA involves two steps: the first is to pretrain an LLM-guided\n",
            "CT autoencoder, which can encode a CT image into continuous high-level features\n",
            "and quantize them into a token space to produce semantic tokens derived from\n",
            "the LLM's vocabulary; and the second is to minimize the discrepancy between the\n",
            "denoised CT images and normal dose CT in terms of both encoded high-level\n",
            "features and quantized token embeddings derived by the LLM-guided CT\n",
            "autoencoder. Extensive experimental results on two public LDCT denoising\n",
            "datasets demonstrate that our LEDA can enhance existing denoising models in\n",
            "terms of quantitative metrics and qualitative evaluat ...\n",
            "\n",
            "### [3] Trainable Joint Bilateral Filters for Enhanced Prediction Stability in Low-dose CT\n",
            "- url: http://arxiv.org/abs/2207.07368v1\n",
            "- published: 2022-07-15\n",
            "- authors: Fabian Wagner, Mareike Thies, Felix Denzinger, Mingxuan Gu, Mayank Patwari, Stefan Ploner, Noah Maul, Laura Pfaff, Yixing Huang, Andreas Maier\n",
            "- content:\n",
            "Low-dose computed tomography (CT) denoising algorithms aim to enable reduced\n",
            "patient dose in routine CT acquisitions while maintaining high image quality.\n",
            "Recently, deep learning~(DL)-based methods were introduced, outperforming\n",
            "conventional denoising algorithms on this task due to their high model\n",
            "capacity. However, for the transition of DL-based denoising to clinical\n",
            "practice, these data-driven approaches must generalize robustly beyond the seen\n",
            "training data. We, therefore, propose a hybrid denoising approach consisting of\n",
            "a set of trainable joint bilateral filters (JBFs) combined with a convolutional\n",
            "DL-based denoising network to predict the guidance image. Our proposed\n",
            "denoising pipeline combines the high model capacity enabled by DL-based feature\n",
            "extraction with the reliability of the conventional JBF. The pipeline's ability\n",
            "to generalize is demonstrated by training on abdomen CT scans without metal\n",
            "implants and testing on abdomen scans with metal implants as well as on head CT\n",
            "data. When embedding two well-established DL-based denoisers (RED-CNN/QAE) in\n",
            "our pipeline, the denoising performance is improved by $10\\,\\%$/$82\\,\\%$ (RMSE)\n",
            "and $3\\,\\%$/$81\\,\\%$ (PSNR) in regions cont ...\n",
            "\n",
            "\n",
            "=== [Summarized Overview + References] ===\n",
            "\n",
            "### CT Denoising Research Landscape Overview\n",
            "\n",
            "CT (Computed Tomography) denoising은 의료 영상에서 발생하는 노이즈를 줄이는 기술로, 특히 낮은 방사선량을 사용한 CT(저선량 CT, LDCT)에서는 더욱 중요합니다. 최근에는 딥러닝 기반의 자가 감독 학습(Self-Supervised Learning) 기법을 통해 데이터 라벨 없이도 CT 이미지의 노이즈 감소를 수행할 수 있는 연구가 활발히 진행되고 있습니다. 이러한 방법들은 저선량 CT 이미지를 개선하고, 관련한 높은 품질의 CT 이미지를 생성하는 데 초점을 맞추고 있습니다. 특히, 복잡한 CT 노이즈의 특성을 처리하고, 모델의 일반화 능력을 향상시키기 위한 다양한 접근 방식이 제안되고 있습니다.\n",
            "\n",
            "현재 진행되고 있는 주요 연구들은:\n",
            "- 자가 감독 학습을 통한 노이즈 모델 예측.\n",
            "- 언어 모델을 활용한 세분화된 고품질 CT 이미지 생성.\n",
            "- 하이브리드 필터 및 딥러닝 네트워크의 결합을 통한 노이즈 저감 성능 향상.\n",
            "\n",
            "이러한 접근은 CT 영상의 품질 향상에 기여하며 환자의 방사선 노출을 최소화하는 데 중요한 역할을 하고 있습니다.\n",
            "\n",
            "### References\n",
            "- [Self-Supervised Learning based CT Denoising using Pseudo-CT Image Pairs](http://arxiv.org/abs/2104.02326v1) — Published: 2021-04-06 — Dongkyu Won, Euijin Jung, Sion An, Philip Chikontwe, Sang Hyun Park\n",
            "- [Low-dose CT Denoising with Language-engaged Dual-space Alignment](http://arxiv.org/abs/2403.06128v1) — Published: 2024-03-10 — Zhihao Chen, Tao Chen, Chenhui Wang, Chuang Niu, Ge Wang, Hongming Shan\n",
            "- [Trainable Joint Bilateral Filters for Enhanced Prediction Stability in Low-dose CT](http://arxiv.org/abs/2207.07368v1) — Published: 2022-07-15 — Fabian Wagner, Mareike Thies, Felix Denzinger, Mingxuan Gu, Mayank Patwari, Stefan Ploner, Noah Maul, Laura Pfaff, Yixing Huang, Andreas Maier\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ask_followup_agent(\"세 번째 논문에서 Bilateral Filter가 어떻게 학습 가능하다는거야? 이는 non-trainable한 필터아냐?\")"
      ],
      "metadata": {
        "id": "zG7fCMhxqJcK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "366b9432-eee6-4420-bbf0-5230825a86f8"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Router] decision = RAG | Trainable Joint Bilateral Filters for Enhanced Prediction Stability in Low-dose CT\n",
            "=== [RAG Answer From Local Chroma] ===\n",
            "\n",
            "세 번째 논문에서는 Bilateral Filter가 학습 가능하다는 것을 어떻게 설명하고 있는지에 대해 설명합니다. 일반적으로 Bilateral Filter는 non-trainable한 필터로 알려져 있지만, 이 연구에서는 \"trainable joint bilateral filter (JBF)\"를 도입하여 이를 해결합니다. 이 JBF는 깊이 신경망에서 사용할 수 있도록 완전히 미분 가능하게 설계되었으며, 입력 이미지 및 가이드 이미지에 대한 예측을 기반으로 필터 파라미터를 학습합니다.\n",
            "\n",
            "특히, JBF는 가이드 이미지를 통해 추가 정보를 고려하여 노이즈 제거를 수행하는데, 이 과정에서 네 개의 kernel width 파라미터가 학습 가능한 가중치로 사용됩니다. 이 필터는 [Trainable Joint Bilateral Filters for Enhanced Prediction Stability in Low-dose CT] 논문에 의해 제안된 것으로, 기존의 Bilateral Filter와 달리 깊이 학습 모델의 일부분으로 통합되어 데이터 기반 최적화가 가능하다는 점에서 차별화됩니다 (chunk 0, chunk 1).\n",
            "\n",
            "이러한 접근법은 기존의 Bilateral Filter의 특징을 유지하면서도 딥러닝 기반 모델의 높은 표현력을 결합하여 이미지 품질 향상을 꾀하며, 필터의 파라미터는 학습 과정 중에 효과적으로 조정될 수 있습니다 (chunk 3).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SPOFdUvSGU3u"
      },
      "execution_count": 9,
      "outputs": []
    }
  ]
}